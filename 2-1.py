from llama_cpp import Tokenizer

tok = Tokenizer("tokenizer.model")

tok.n_words