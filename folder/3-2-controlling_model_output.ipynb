{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling Model Output\n",
    "\n",
    "## Stopping Token Generation\n",
    "\n",
    "Controlling the output of the language model is a difficult task and, as you saw in a previous lecture, we can use few\n",
    "shot prompting to help with some of the formatting requirements, but it can be a bit tricky to have the model just stop\n",
    "after one item. Thankfully, llama.cpp has a couple of other mechanisms we can leverage to improve our control over the\n",
    "model output. The first of these is a stopping criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from /home/brooksch/sandboxes/llama/llama-2-13b/ggml-model-f16-q5_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  8801.63 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   400.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =    75.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# Let's load and configure our llama 2 model\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model: Llama = Llama(model_path=os.environ[\"LLAMA_13B\"], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The question was asked by a Buddhist monk in an attempt to understand the nature of mind"
     ]
    }
   ],
   "source": [
    "# The most blunt way to stop the LLM generation is to indicate\n",
    "# the token or tokens you want to break on in a list when calling\n",
    "# create_completion(). Here I'll ask for the continuation of this\n",
    "# prompt, but I only want a single sentence so I'll stop on a\n",
    "# period or other select punctuation.\n",
    "\n",
    "token_count: int = 0\n",
    "for result in model.create_completion(\n",
    "    prompt=\"What is the sound of one hand clapping?\",\n",
    "    max_tokens=512,\n",
    "    stream=True,\n",
    "    stop=[\".\", \"?\", \"!\"],\n",
    "):\n",
    "    if token_count % 50 == 0:\n",
    "        print(\"\")\n",
    "    token_count = token_count + 1\n",
    "    print(result[\"choices\"][0][\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this form of stopping is actually done in the python library itself, and not the underlying llama.cpp. There is\n",
    "also an option to set a callback function to determine when to stop as well, and this is through the `stopping_criteria`\n",
    "parameter.\n",
    "\n",
    "I have a confess, this is a source of frustration that I have with the tools we are using. While the tools work well,\n",
    "the docs sometimes lag, or are unclear. The newness of the tools -- and the lack of a clear standard or even dominant\n",
    "tool for development in this space -- also makes it a bit of work sometimes to understand whats happening under the\n",
    "hood.\n",
    "\n",
    "If we go to the documentation on the `llama-cpp-python` bindings website, we see that the `stopping_criteria` is\n",
    "actually a list of callable objects -- so functions -- which take in two parameters, both numpy arrays. The first\n",
    "parameter is an array of unsigned int's and the second an array of single precision float values. The expectation is\n",
    "that the callback function will return a boolean, presumably indicating whether we should stop processing or not.\n",
    "\n",
    "I think this is a bit easier to understand if we just use the debugger to take a look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The sound of one hand clapping is a metaphor for something that cannot be heard. It is often used to describe an idea or concept that cannot be expressed in words. The phrase can also be used to describe a situation where there is\n",
      " no response from someone who has been asked a question.\n",
      "What does the sound of one hand clapping mean?\n",
      "The sound of one hand clapping is a metaphor for something that cannot be heard. It is often used to describe an\n",
      " idea or concept that cannot be expressed in words. The phrase can also be used to describe a situation where there is no response from someone who has been asked a question.\n",
      "What does the sound of one hand clapping mean in Buddhism?\n",
      "\n",
      "The sound of one hand clapping is a metaphor for something that cannot be heard. It is often used to describe an idea or concept that cannot be expressed in words. The phrase can also be used to describe a situation where there is no\n",
      " response from someone who has been asked a question. In Buddhism, the sound of one hand clapping represents the emptiness of all things. This includes the emptiness of our thoughts and feelings, as well as the emptiness of\n",
      " the world around us. The Buddha taught that everything in this world is impermanent and subject to change. This includes our thoughts, feelings, and even the physical world itself. Everything is constantly changing and nothing can be held onto forever.\n",
      "\n",
      "The sound of one hand clapping is a metaphor for something that cannot be heard. It is often used to describe an idea or concept that cannot be expressed in words. The phrase can also be used to describe a situation where there is\n",
      " no response from someone who has been asked a question. In Buddhism, the sound of one hand clapping represents the emptiness of all things. This includes the emptiness of our thoughts and feelings, as well as the emptiness\n",
      " of the world around us. The Buddha taught that everything in this world is impermanent and subject to change. This includes our thoughts, feelings, and even the physical world itself. Everything is constantly changing and nothing can be held onto forever\n",
      ".\n",
      "What does the sound of one hand clapping mean in Zen Buddhism?\n",
      "The sound of one hand clapping is a metaphor for something that cannot be heard. It is often used to describe an idea or concept that cannot\n",
      " be"
     ]
    }
   ],
   "source": [
    "# Let's define the function we are going to use as our\n",
    "# stopping criteria. At the moment I'm just going to\n",
    "# return false, and set a breakpoint on this line\n",
    "def should_stop(input_ids, logits) -> bool:\n",
    "    return False\n",
    "\n",
    "\n",
    "# Now we'll do the rest as we did previously, this\n",
    "# time setting up our stopping_criteria callback. I want\n",
    "# to use a temperature of zero just for a demonstration\n",
    "token_count: int = 0\n",
    "for result in model.create_completion(\n",
    "    prompt=\"What is the sound of one hand clapping?\",\n",
    "    max_tokens=512,\n",
    "    stream=True,\n",
    "    temperature=0,\n",
    "    stopping_criteria=should_stop,\n",
    "):\n",
    "    if token_count % 50 == 0:\n",
    "        print(\"\")\n",
    "    token_count = token_count + 1\n",
    "    print(result[\"choices\"][0][\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, to debug the cell in vs code I'm going to set a breakpoint in the margin on line five. Then we can hit the drop down\n",
    "on the left hand side to run the cell in debug mode.\n",
    "\n",
    "In the debug panel to the left we can see our local variables -- the `input_ids` parameter, and if we expand that we can\n",
    "see it has 11 values in it, all numbers. Take a minute to think about this -- what do you think those numbers are?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that is our sequence of tokens. If we expand the globals section of the debugger we see that the llama\n",
    "object is there as `model`. We looked at tokenization previously, and the model object has a handy function to\n",
    "detokenize a sequence, converting it back into the text representation which is more familiar to us.\n",
    "\n",
    "To do that we have a couple of options available to us, and I'm going to show you both just in case you are unfamiliar\n",
    "with the vs code debugging facilities. The first is to add a watch variable. This is some expression which will persist\n",
    "across various debugging runs, and is a hand method to use when debugging loops. I'm going to add the watch expression\n",
    "as `model.detokenize(input_ids)`. Once I do this it becomes clear that this is our prompt!\n",
    "\n",
    "Another option is to use the debug console at the bottom of the window. This allows you to write arbitrary python to\n",
    "interact with the interpreter. If I put in the same code here, `model.detokenize(input_ids)`, we get the same result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this isn't at all unique to llama 2, but it's an important skill in when you want to understand new and emerging\n",
    "packages and the documentation is lacking. If you haven't seen this before, I encourage you to try it out, and perhaps\n",
    "look up a few videos on debugging in vs code. If you can master your toolkit, your ability to solve problems will\n",
    "increase two fold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our function. We're returning `False` here, which will indicate that the stream of tokens has not yet\n",
    "met a stopping criteria. We can hit the debug continue button, or F5, a few times to go through a few breakpoint\n",
    "iterations. As we do this, we can see that the sequence grows in size, and that the detokenization of it is giving us\n",
    "the prompt plus the continuation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn our attention to the logits. The logits is huge, 32,000 items, so you can imagine this aligns with our\n",
    "vocabulary. Each of the values in the logits array is the log odds that a given token will be the right one for the next\n",
    "response. Don't let this scare you away, the log odds -- a logit -- is basically just a probability value that ranges\n",
    "from negative infinity to positive infinity.\n",
    "\n",
    "Let's open the debug console and see what some of the best logits are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not going to go into the details about how this mapping is done, but what we want are the values which are closest\n",
    "to zero. These are our most likely candidates, and the values which are out towards positive or negative infinity are\n",
    "our least likely candidates.\n",
    "\n",
    "Do do this I'm going to `import numpy as np` first, then I'm going to take the absolute value of the logits array, sort\n",
    "it by size, then take the last five items -- which will be our five closest to zero -- and detokenize them to see if it\n",
    "makes sense.\n",
    "\n",
    "Keep in mind that when you see the string output, it's actually in reverse order. The most likely token is the very last\n",
    "one, so we can slice the last five items using numpy slicing syntax to reverse them.\n",
    "\n",
    "Here's the code for that: `model.detokenize(np.absolute(logits).argsort()[-5:][::-1])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Ok, in this lecture I wanted to show you two different ways you can control the output of the LLM, the first is through\n",
    "a sequence of stopping items provided as a list, and the second is through the `stopping_criteria` where you can provide\n",
    "multiple different callback functions which operate on the underlying probabilities of the model responses. Both are\n",
    "useful, and you can imagine that with a function in the `stopping_criteria` you can do anything you might want to do in\n",
    "python, including validating that the response fits a specific regular expression pattern, or even raising a partially\n",
    "completed prompt to another service, process, or person to validate.\n",
    "\n",
    "Along the way I demonstrated how you can use the vs code environment to dig into the underlying operation of the LLM\n",
    "and, frankly, I just showed a simple example! Once you have mastered debugging skills within the platform, you can\n",
    "quickly pick up software packages like llama.cpp's python bindings by just stepping through the code to see how it\n",
    "works. I think this is an incredibly important developer skill that often we don't do a good job teaching, so I hope\n",
    "you'll consider exploring this on your own to better understand how the llama.cpp package works.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
